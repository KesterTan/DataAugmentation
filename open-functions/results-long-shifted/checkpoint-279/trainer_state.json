{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.970666666666667,
  "eval_steps": 500,
  "global_step": 279,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010666666666666666,
      "grad_norm": Infinity,
      "learning_rate": 0.0,
      "loss": 51.3431,
      "step": 1
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 79.02726745605469,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 46.4002,
      "step": 2
    },
    {
      "epoch": 0.032,
      "grad_norm": 87.14811706542969,
      "learning_rate": 4.000000000000001e-06,
      "loss": 49.8458,
      "step": 3
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 83.78641510009766,
      "learning_rate": 6e-06,
      "loss": 47.9709,
      "step": 4
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 82.34961700439453,
      "learning_rate": 8.000000000000001e-06,
      "loss": 46.9032,
      "step": 5
    },
    {
      "epoch": 0.064,
      "grad_norm": 92.59580993652344,
      "learning_rate": 1e-05,
      "loss": 49.812,
      "step": 6
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 89.0328369140625,
      "learning_rate": 1.2e-05,
      "loss": 47.0789,
      "step": 7
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 98.28575897216797,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 48.9806,
      "step": 8
    },
    {
      "epoch": 0.096,
      "grad_norm": 108.2145767211914,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 49.6852,
      "step": 9
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 103.64386749267578,
      "learning_rate": 1.8e-05,
      "loss": 45.2633,
      "step": 10
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 114.71145629882812,
      "learning_rate": 2e-05,
      "loss": 45.6546,
      "step": 11
    },
    {
      "epoch": 0.128,
      "grad_norm": 121.73867797851562,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 43.5852,
      "step": 12
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 137.52960205078125,
      "learning_rate": 2.4e-05,
      "loss": 44.367,
      "step": 13
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 136.98826599121094,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 40.451,
      "step": 14
    },
    {
      "epoch": 0.16,
      "grad_norm": 146.74542236328125,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 38.1284,
      "step": 15
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": Infinity,
      "learning_rate": 3e-05,
      "loss": 40.4365,
      "step": 16
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 176.53115844726562,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 39.9863,
      "step": 17
    },
    {
      "epoch": 0.192,
      "grad_norm": 189.5316619873047,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 37.6228,
      "step": 18
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 193.25320434570312,
      "learning_rate": 3.6e-05,
      "loss": 34.3372,
      "step": 19
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 209.87522888183594,
      "learning_rate": 3.8e-05,
      "loss": 33.0552,
      "step": 20
    },
    {
      "epoch": 0.224,
      "grad_norm": 201.75498962402344,
      "learning_rate": 4e-05,
      "loss": 28.962,
      "step": 21
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 190.3367156982422,
      "learning_rate": 4.2e-05,
      "loss": 25.3259,
      "step": 22
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 216.73165893554688,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 25.5146,
      "step": 23
    },
    {
      "epoch": 0.256,
      "grad_norm": 188.82806396484375,
      "learning_rate": 4.600000000000001e-05,
      "loss": 20.9871,
      "step": 24
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 201.39088439941406,
      "learning_rate": 4.8e-05,
      "loss": 20.2765,
      "step": 25
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 190.88589477539062,
      "learning_rate": 5e-05,
      "loss": 17.7774,
      "step": 26
    },
    {
      "epoch": 0.288,
      "grad_norm": 194.51490783691406,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 16.2624,
      "step": 27
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 166.73684692382812,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 13.4467,
      "step": 28
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 165.86839294433594,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 11.9253,
      "step": 29
    },
    {
      "epoch": 0.32,
      "grad_norm": 177.37533569335938,
      "learning_rate": 5.8e-05,
      "loss": 11.0524,
      "step": 30
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 175.60861206054688,
      "learning_rate": 6e-05,
      "loss": 9.738,
      "step": 31
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 183.58543395996094,
      "learning_rate": 6.2e-05,
      "loss": 8.6931,
      "step": 32
    },
    {
      "epoch": 0.352,
      "grad_norm": 171.47584533691406,
      "learning_rate": 6.400000000000001e-05,
      "loss": 7.397,
      "step": 33
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 170.2588653564453,
      "learning_rate": 6.6e-05,
      "loss": 6.4077,
      "step": 34
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 143.11788940429688,
      "learning_rate": 6.800000000000001e-05,
      "loss": 5.4572,
      "step": 35
    },
    {
      "epoch": 0.384,
      "grad_norm": 116.03388214111328,
      "learning_rate": 7e-05,
      "loss": 4.8124,
      "step": 36
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 56.48487091064453,
      "learning_rate": 7.2e-05,
      "loss": 4.3832,
      "step": 37
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 29.592588424682617,
      "learning_rate": 7.4e-05,
      "loss": 4.1677,
      "step": 38
    },
    {
      "epoch": 0.416,
      "grad_norm": 22.41568374633789,
      "learning_rate": 7.6e-05,
      "loss": 4.0085,
      "step": 39
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 21.292402267456055,
      "learning_rate": 7.800000000000001e-05,
      "loss": 3.6609,
      "step": 40
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 22.485225677490234,
      "learning_rate": 8e-05,
      "loss": 3.4247,
      "step": 41
    },
    {
      "epoch": 0.448,
      "grad_norm": 25.689218521118164,
      "learning_rate": 8.2e-05,
      "loss": 3.0562,
      "step": 42
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 28.048566818237305,
      "learning_rate": 8.4e-05,
      "loss": 2.7919,
      "step": 43
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 30.951221466064453,
      "learning_rate": 8.6e-05,
      "loss": 2.3609,
      "step": 44
    },
    {
      "epoch": 0.48,
      "grad_norm": 27.904409408569336,
      "learning_rate": 8.800000000000001e-05,
      "loss": 2.1093,
      "step": 45
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 24.231658935546875,
      "learning_rate": 9e-05,
      "loss": 1.6685,
      "step": 46
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 18.717424392700195,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.3942,
      "step": 47
    },
    {
      "epoch": 0.512,
      "grad_norm": 9.827118873596191,
      "learning_rate": 9.4e-05,
      "loss": 1.2939,
      "step": 48
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 4.961047649383545,
      "learning_rate": 9.6e-05,
      "loss": 1.0693,
      "step": 49
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 3.1369476318359375,
      "learning_rate": 9.8e-05,
      "loss": 0.9551,
      "step": 50
    },
    {
      "epoch": 0.544,
      "grad_norm": 2.3266613483428955,
      "learning_rate": 0.0001,
      "loss": 0.9627,
      "step": 51
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 3.055349349975586,
      "learning_rate": 0.00010200000000000001,
      "loss": 1.069,
      "step": 52
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 1.2293683290481567,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.7697,
      "step": 53
    },
    {
      "epoch": 0.576,
      "grad_norm": 1.4588502645492554,
      "learning_rate": 0.00010600000000000002,
      "loss": 0.9256,
      "step": 54
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 1.1654675006866455,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.7439,
      "step": 55
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.7910487055778503,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.7085,
      "step": 56
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.0617400407791138,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.7684,
      "step": 57
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.8665860891342163,
      "learning_rate": 0.00011399999999999999,
      "loss": 0.7034,
      "step": 58
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.8672811388969421,
      "learning_rate": 0.000116,
      "loss": 0.6518,
      "step": 59
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.319786787033081,
      "learning_rate": 0.000118,
      "loss": 0.64,
      "step": 60
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 1.125067949295044,
      "learning_rate": 0.00012,
      "loss": 0.6193,
      "step": 61
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 1.0491408109664917,
      "learning_rate": 0.000122,
      "loss": 0.6046,
      "step": 62
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.9538508057594299,
      "learning_rate": 0.000124,
      "loss": 0.6219,
      "step": 63
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 1.284347653388977,
      "learning_rate": 0.000126,
      "loss": 0.7204,
      "step": 64
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.7597243785858154,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.4724,
      "step": 65
    },
    {
      "epoch": 0.704,
      "grad_norm": 1.156153917312622,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.5959,
      "step": 66
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.8413311243057251,
      "learning_rate": 0.000132,
      "loss": 0.4746,
      "step": 67
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.8114013671875,
      "learning_rate": 0.000134,
      "loss": 0.5904,
      "step": 68
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.7491260170936584,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.5485,
      "step": 69
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.6892296671867371,
      "learning_rate": 0.000138,
      "loss": 0.4371,
      "step": 70
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.7059453129768372,
      "learning_rate": 0.00014,
      "loss": 0.5348,
      "step": 71
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.5681307911872864,
      "learning_rate": 0.000142,
      "loss": 0.3592,
      "step": 72
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.7649906873703003,
      "learning_rate": 0.000144,
      "loss": 0.6103,
      "step": 73
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.8318929672241211,
      "learning_rate": 0.000146,
      "loss": 0.4234,
      "step": 74
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.7901983857154846,
      "learning_rate": 0.000148,
      "loss": 0.5871,
      "step": 75
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.5221580266952515,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.4547,
      "step": 76
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.8841480016708374,
      "learning_rate": 0.000152,
      "loss": 0.4696,
      "step": 77
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.5314134359359741,
      "learning_rate": 0.000154,
      "loss": 0.4335,
      "step": 78
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.6218701601028442,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.3258,
      "step": 79
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.6477568745613098,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.4666,
      "step": 80
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.5700717568397522,
      "learning_rate": 0.00016,
      "loss": 0.3497,
      "step": 81
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.6808046698570251,
      "learning_rate": 0.000162,
      "loss": 0.3805,
      "step": 82
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.6306189298629761,
      "learning_rate": 0.000164,
      "loss": 0.4383,
      "step": 83
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.6638657450675964,
      "learning_rate": 0.000166,
      "loss": 0.3565,
      "step": 84
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.5670750141143799,
      "learning_rate": 0.000168,
      "loss": 0.48,
      "step": 85
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.47907140851020813,
      "learning_rate": 0.00017,
      "loss": 0.4065,
      "step": 86
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.5389004945755005,
      "learning_rate": 0.000172,
      "loss": 0.3912,
      "step": 87
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.5096572041511536,
      "learning_rate": 0.000174,
      "loss": 0.501,
      "step": 88
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.6055352687835693,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.3558,
      "step": 89
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.6217848062515259,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.4842,
      "step": 90
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.5900298953056335,
      "learning_rate": 0.00018,
      "loss": 0.4244,
      "step": 91
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.4905893802642822,
      "learning_rate": 0.000182,
      "loss": 0.436,
      "step": 92
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.47289732098579407,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.4273,
      "step": 93
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.3448958098888397,
      "learning_rate": 0.00018600000000000002,
      "loss": 0.3329,
      "step": 94
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.40777865052223206,
      "learning_rate": 0.000188,
      "loss": 0.3561,
      "step": 95
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.39576810598373413,
      "learning_rate": 0.00019,
      "loss": 0.3595,
      "step": 96
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.48985767364501953,
      "learning_rate": 0.000192,
      "loss": 0.3665,
      "step": 97
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.5228135585784912,
      "learning_rate": 0.000194,
      "loss": 0.3311,
      "step": 98
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.6467090845108032,
      "learning_rate": 0.000196,
      "loss": 0.3931,
      "step": 99
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.6582112908363342,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.3488,
      "step": 100
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.5189732909202576,
      "learning_rate": 0.0002,
      "loss": 0.3737,
      "step": 101
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.7358540296554565,
      "learning_rate": 0.00019998459887166634,
      "loss": 0.3921,
      "step": 102
    },
    {
      "epoch": 1.096,
      "grad_norm": 1.1099215745925903,
      "learning_rate": 0.00019993840023056043,
      "loss": 0.4134,
      "step": 103
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.8942726850509644,
      "learning_rate": 0.00019986141830690625,
      "loss": 0.3291,
      "step": 104
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.5150142312049866,
      "learning_rate": 0.00019975367681287356,
      "loss": 0.2874,
      "step": 105
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.6688517332077026,
      "learning_rate": 0.00019961520893527383,
      "loss": 0.3726,
      "step": 106
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.4591550827026367,
      "learning_rate": 0.00019944605732533818,
      "loss": 0.4068,
      "step": 107
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.49927273392677307,
      "learning_rate": 0.00019924627408557963,
      "loss": 0.3232,
      "step": 108
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.566314160823822,
      "learning_rate": 0.00019901592075374447,
      "loss": 0.4636,
      "step": 109
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.510438859462738,
      "learning_rate": 0.00019875506828385722,
      "loss": 0.299,
      "step": 110
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.47867751121520996,
      "learning_rate": 0.00019846379702436517,
      "loss": 0.287,
      "step": 111
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.6430663466453552,
      "learning_rate": 0.00019814219669338928,
      "loss": 0.2951,
      "step": 112
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.4585312008857727,
      "learning_rate": 0.0001977903663510889,
      "loss": 0.2929,
      "step": 113
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.562740683555603,
      "learning_rate": 0.00019740841436914917,
      "loss": 0.2677,
      "step": 114
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.533262312412262,
      "learning_rate": 0.00019699645839739985,
      "loss": 0.3496,
      "step": 115
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.5839688181877136,
      "learning_rate": 0.00019655462532757676,
      "loss": 0.2564,
      "step": 116
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.6982330679893494,
      "learning_rate": 0.00019608305125423607,
      "loss": 0.3217,
      "step": 117
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.614714503288269,
      "learning_rate": 0.00019558188143283426,
      "loss": 0.2947,
      "step": 118
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.6567665338516235,
      "learning_rate": 0.000195051270234986,
      "loss": 0.4163,
      "step": 119
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.48993805050849915,
      "learning_rate": 0.00019449138110091445,
      "loss": 0.3967,
      "step": 120
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.5574067831039429,
      "learning_rate": 0.00019390238648910765,
      "loss": 0.3817,
      "step": 121
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.5581551194190979,
      "learning_rate": 0.00019328446782319772,
      "loss": 0.396,
      "step": 122
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.45281773805618286,
      "learning_rate": 0.00019263781543607815,
      "loss": 0.2758,
      "step": 123
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.5749281048774719,
      "learning_rate": 0.00019196262851127693,
      "loss": 0.3043,
      "step": 124
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.840930163860321,
      "learning_rate": 0.00019125911502160364,
      "loss": 0.3116,
      "step": 125
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 1.3940532207489014,
      "learning_rate": 0.00019052749166508908,
      "loss": 0.3474,
      "step": 126
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.540797770023346,
      "learning_rate": 0.0001897679837982373,
      "loss": 0.2801,
      "step": 127
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.8248295783996582,
      "learning_rate": 0.00018898082536661094,
      "loss": 0.2526,
      "step": 128
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.624501645565033,
      "learning_rate": 0.00018816625883277043,
      "loss": 0.3665,
      "step": 129
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.6718758940696716,
      "learning_rate": 0.00018732453510159026,
      "loss": 0.3249,
      "step": 130
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.5093392133712769,
      "learning_rate": 0.0001864559134429745,
      "loss": 0.2757,
      "step": 131
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.48827242851257324,
      "learning_rate": 0.000185560661411996,
      "loss": 0.3103,
      "step": 132
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.6097645163536072,
      "learning_rate": 0.00018463905476648307,
      "loss": 0.2735,
      "step": 133
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.5222679376602173,
      "learning_rate": 0.0001836913773820802,
      "loss": 0.2455,
      "step": 134
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.5501309633255005,
      "learning_rate": 0.00018271792116480765,
      "loss": 0.3061,
      "step": 135
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.5494089722633362,
      "learning_rate": 0.00018171898596114805,
      "loss": 0.3243,
      "step": 136
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.43815362453460693,
      "learning_rate": 0.00018069487946568673,
      "loss": 0.2265,
      "step": 137
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.440899133682251,
      "learning_rate": 0.00017964591712633498,
      "loss": 0.2745,
      "step": 138
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.5327789783477783,
      "learning_rate": 0.00017857242204716495,
      "loss": 0.2166,
      "step": 139
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.6741783618927002,
      "learning_rate": 0.00017747472488888622,
      "loss": 0.3165,
      "step": 140
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.8549733757972717,
      "learning_rate": 0.0001763531637669949,
      "loss": 0.1927,
      "step": 141
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.8225542902946472,
      "learning_rate": 0.00017520808414762641,
      "loss": 0.2697,
      "step": 142
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.6043880581855774,
      "learning_rate": 0.0001740398387411442,
      "loss": 0.3526,
      "step": 143
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.586249828338623,
      "learning_rate": 0.0001728487873934969,
      "loss": 0.3508,
      "step": 144
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.4339207410812378,
      "learning_rate": 0.00017163529697537754,
      "loss": 0.275,
      "step": 145
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.5439775586128235,
      "learning_rate": 0.00017039974126921946,
      "loss": 0.2531,
      "step": 146
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.5076203346252441,
      "learning_rate": 0.0001691425008540625,
      "loss": 0.2559,
      "step": 147
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.4836730360984802,
      "learning_rate": 0.00016786396298832624,
      "loss": 0.3052,
      "step": 148
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.5261409282684326,
      "learning_rate": 0.0001665645214905257,
      "loss": 0.3151,
      "step": 149
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.5664762258529663,
      "learning_rate": 0.00016524457661796626,
      "loss": 0.2277,
      "step": 150
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.7357369661331177,
      "learning_rate": 0.0001639045349434554,
      "loss": 0.3186,
      "step": 151
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.648063600063324,
      "learning_rate": 0.00016254480923006925,
      "loss": 0.3822,
      "step": 152
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.4703541398048401,
      "learning_rate": 0.00016116581830401193,
      "loss": 0.3142,
      "step": 153
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.71302729845047,
      "learning_rate": 0.00015976798692560795,
      "loss": 0.3172,
      "step": 154
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.5259230732917786,
      "learning_rate": 0.00015835174565846622,
      "loss": 0.1933,
      "step": 155
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.5402661561965942,
      "learning_rate": 0.00015691753073685693,
      "loss": 0.2908,
      "step": 156
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.5728961229324341,
      "learning_rate": 0.0001554657839313413,
      "loss": 0.2294,
      "step": 157
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.5467243194580078,
      "learning_rate": 0.00015399695241269666,
      "loss": 0.1533,
      "step": 158
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.8194478750228882,
      "learning_rate": 0.00015251148861417733,
      "loss": 0.3072,
      "step": 159
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.6560313105583191,
      "learning_rate": 0.0001510098500921552,
      "loss": 0.243,
      "step": 160
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.5972570180892944,
      "learning_rate": 0.000149492499385182,
      "loss": 0.212,
      "step": 161
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.6654322743415833,
      "learning_rate": 0.00014795990387151718,
      "loss": 0.2917,
      "step": 162
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.5898211598396301,
      "learning_rate": 0.0001464125356251644,
      "loss": 0.2778,
      "step": 163
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.5988537669181824,
      "learning_rate": 0.00014485087127046254,
      "loss": 0.2273,
      "step": 164
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.6381375789642334,
      "learning_rate": 0.00014327539183527446,
      "loss": 0.2001,
      "step": 165
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.5515479445457458,
      "learning_rate": 0.00014168658260281945,
      "loss": 0.2929,
      "step": 166
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.5145713686943054,
      "learning_rate": 0.0001400849329621953,
      "loss": 0.3101,
      "step": 167
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.5395941734313965,
      "learning_rate": 0.00013847093625763516,
      "loss": 0.2761,
      "step": 168
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.44635429978370667,
      "learning_rate": 0.0001368450896365467,
      "loss": 0.2974,
      "step": 169
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.45206189155578613,
      "learning_rate": 0.000135207893896379,
      "loss": 0.2709,
      "step": 170
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.4343152642250061,
      "learning_rate": 0.0001335598533303662,
      "loss": 0.2201,
      "step": 171
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.5755834579467773,
      "learning_rate": 0.00013190147557219338,
      "loss": 0.1278,
      "step": 172
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.9410521984100342,
      "learning_rate": 0.00013023327143963414,
      "loss": 0.3154,
      "step": 173
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 1.2652251720428467,
      "learning_rate": 0.0001285557547772072,
      "loss": 0.2619,
      "step": 174
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.45879125595092773,
      "learning_rate": 0.00012686944229790042,
      "loss": 0.1603,
      "step": 175
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.9062473773956299,
      "learning_rate": 0.00012517485342401202,
      "loss": 0.289,
      "step": 176
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.6038976311683655,
      "learning_rate": 0.00012347251012715627,
      "loss": 0.3073,
      "step": 177
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.6077815890312195,
      "learning_rate": 0.00012176293676748493,
      "loss": 0.1186,
      "step": 178
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.7139812707901001,
      "learning_rate": 0.0001200466599321721,
      "loss": 0.206,
      "step": 179
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.714497983455658,
      "learning_rate": 0.00011832420827321373,
      "loss": 0.2891,
      "step": 180
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.5998792052268982,
      "learning_rate": 0.0001165961123445908,
      "loss": 0.2785,
      "step": 181
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.6196197271347046,
      "learning_rate": 0.00011486290443884666,
      "loss": 0.2038,
      "step": 182
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.6830497980117798,
      "learning_rate": 0.00011312511842312909,
      "loss": 0.166,
      "step": 183
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.6299752593040466,
      "learning_rate": 0.00011138328957474691,
      "loss": 0.2132,
      "step": 184
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.5738945603370667,
      "learning_rate": 0.00010963795441629274,
      "loss": 0.3396,
      "step": 185
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.5656545758247375,
      "learning_rate": 0.00010788965055038178,
      "loss": 0.1865,
      "step": 186
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.6200366020202637,
      "learning_rate": 0.00010613891649405815,
      "loss": 0.179,
      "step": 187
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.7458531856536865,
      "learning_rate": 0.00010438629151291943,
      "loss": 0.1857,
      "step": 188
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 0.665373682975769,
      "learning_rate": 0.00010263231545501068,
      "loss": 0.125,
      "step": 189
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.7207174897193909,
      "learning_rate": 0.00010087752858453923,
      "loss": 0.2284,
      "step": 190
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.5853610634803772,
      "learning_rate": 9.912247141546079e-05,
      "loss": 0.1572,
      "step": 191
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.6896594762802124,
      "learning_rate": 9.736768454498936e-05,
      "loss": 0.242,
      "step": 192
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.469781756401062,
      "learning_rate": 9.56137084870806e-05,
      "loss": 0.1764,
      "step": 193
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.6145760416984558,
      "learning_rate": 9.38610835059419e-05,
      "loss": 0.1921,
      "step": 194
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.6184295415878296,
      "learning_rate": 9.211034944961823e-05,
      "loss": 0.1418,
      "step": 195
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.5027800798416138,
      "learning_rate": 9.036204558370725e-05,
      "loss": 0.2131,
      "step": 196
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.5830689072608948,
      "learning_rate": 8.861671042525311e-05,
      "loss": 0.2279,
      "step": 197
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.6069842576980591,
      "learning_rate": 8.68748815768709e-05,
      "loss": 0.1967,
      "step": 198
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.6611475348472595,
      "learning_rate": 8.513709556115335e-05,
      "loss": 0.2097,
      "step": 199
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.6242856979370117,
      "learning_rate": 8.340388765540923e-05,
      "loss": 0.2476,
      "step": 200
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 0.5126485824584961,
      "learning_rate": 8.16757917267863e-05,
      "loss": 0.1787,
      "step": 201
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.5811110138893127,
      "learning_rate": 7.995334006782793e-05,
      "loss": 0.1345,
      "step": 202
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.5413945913314819,
      "learning_rate": 7.823706323251512e-05,
      "loss": 0.2516,
      "step": 203
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.4850989878177643,
      "learning_rate": 7.652748987284375e-05,
      "loss": 0.2351,
      "step": 204
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.5602657794952393,
      "learning_rate": 7.482514657598799e-05,
      "loss": 0.1774,
      "step": 205
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.49669960141181946,
      "learning_rate": 7.31305577020996e-05,
      "loss": 0.2318,
      "step": 206
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.5288170576095581,
      "learning_rate": 7.144424522279283e-05,
      "loss": 0.1754,
      "step": 207
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.4693860113620758,
      "learning_rate": 6.976672856036585e-05,
      "loss": 0.2141,
      "step": 208
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.4638051986694336,
      "learning_rate": 6.809852442780664e-05,
      "loss": 0.1908,
      "step": 209
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.38535889983177185,
      "learning_rate": 6.644014666963386e-05,
      "loss": 0.2703,
      "step": 210
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.48880037665367126,
      "learning_rate": 6.479210610362103e-05,
      "loss": 0.1878,
      "step": 211
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.45783188939094543,
      "learning_rate": 6.315491036345338e-05,
      "loss": 0.3344,
      "step": 212
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.4811842739582062,
      "learning_rate": 6.152906374236485e-05,
      "loss": 0.1882,
      "step": 213
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.43137627840042114,
      "learning_rate": 5.991506703780475e-05,
      "loss": 0.2113,
      "step": 214
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.3814865052700043,
      "learning_rate": 5.831341739718055e-05,
      "loss": 0.202,
      "step": 215
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.4595949351787567,
      "learning_rate": 5.672460816472556e-05,
      "loss": 0.2341,
      "step": 216
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.3368566334247589,
      "learning_rate": 5.5149128729537457e-05,
      "loss": 0.1485,
      "step": 217
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.41507452726364136,
      "learning_rate": 5.3587464374835596e-05,
      "loss": 0.1838,
      "step": 218
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.4211174249649048,
      "learning_rate": 5.2040096128482876e-05,
      "loss": 0.2009,
      "step": 219
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.40657612681388855,
      "learning_rate": 5.0507500614817995e-05,
      "loss": 0.1502,
      "step": 220
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.41855672001838684,
      "learning_rate": 4.899014990784485e-05,
      "loss": 0.2708,
      "step": 221
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.4541938602924347,
      "learning_rate": 4.748851138582269e-05,
      "loss": 0.1868,
      "step": 222
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.4777023196220398,
      "learning_rate": 4.6003047587303374e-05,
      "loss": 0.2042,
      "step": 223
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.42658573389053345,
      "learning_rate": 4.453421606865868e-05,
      "loss": 0.1173,
      "step": 224
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.4899316430091858,
      "learning_rate": 4.3082469263143065e-05,
      "loss": 0.211,
      "step": 225
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.3991665542125702,
      "learning_rate": 4.164825434153381e-05,
      "loss": 0.2318,
      "step": 226
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.4678627550601959,
      "learning_rate": 4.0232013074392064e-05,
      "loss": 0.1917,
      "step": 227
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.41699451208114624,
      "learning_rate": 3.8834181695988084e-05,
      "loss": 0.1322,
      "step": 228
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 0.5382829308509827,
      "learning_rate": 3.745519076993078e-05,
      "loss": 0.113,
      "step": 229
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.3765067458152771,
      "learning_rate": 3.609546505654462e-05,
      "loss": 0.1798,
      "step": 230
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.5229018330574036,
      "learning_rate": 3.475542338203377e-05,
      "loss": 0.1381,
      "step": 231
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 0.38826242089271545,
      "learning_rate": 3.343547850947434e-05,
      "loss": 0.1893,
      "step": 232
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.5044072270393372,
      "learning_rate": 3.21360370116738e-05,
      "loss": 0.102,
      "step": 233
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.3726540803909302,
      "learning_rate": 3.085749914593752e-05,
      "loss": 0.1628,
      "step": 234
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.4531090557575226,
      "learning_rate": 2.9600258730780562e-05,
      "loss": 0.2819,
      "step": 235
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.46754321455955505,
      "learning_rate": 2.8364703024622473e-05,
      "loss": 0.0928,
      "step": 236
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.47591885924339294,
      "learning_rate": 2.715121260650316e-05,
      "loss": 0.1696,
      "step": 237
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.4388672113418579,
      "learning_rate": 2.596016125885581e-05,
      "loss": 0.1607,
      "step": 238
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.3666393458843231,
      "learning_rate": 2.4791915852373605e-05,
      "loss": 0.1746,
      "step": 239
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.42938703298568726,
      "learning_rate": 2.364683623300513e-05,
      "loss": 0.1737,
      "step": 240
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 0.44795361161231995,
      "learning_rate": 2.2525275111113807e-05,
      "loss": 0.1344,
      "step": 241
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.3544330894947052,
      "learning_rate": 2.142757795283504e-05,
      "loss": 0.102,
      "step": 242
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.540035605430603,
      "learning_rate": 2.0354082873665015e-05,
      "loss": 0.0985,
      "step": 243
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.2994166314601898,
      "learning_rate": 1.9305120534313293e-05,
      "loss": 0.1122,
      "step": 244
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.47284436225891113,
      "learning_rate": 1.8281014038851963e-05,
      "loss": 0.1006,
      "step": 245
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.3786488175392151,
      "learning_rate": 1.728207883519236e-05,
      "loss": 0.2403,
      "step": 246
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 0.37598565220832825,
      "learning_rate": 1.630862261791982e-05,
      "loss": 0.1628,
      "step": 247
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.36250483989715576,
      "learning_rate": 1.536094523351693e-05,
      "loss": 0.1356,
      "step": 248
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.37836533784866333,
      "learning_rate": 1.4439338588004004e-05,
      "loss": 0.1427,
      "step": 249
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.3371196687221527,
      "learning_rate": 1.3544086557025493e-05,
      "loss": 0.227,
      "step": 250
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.39913323521614075,
      "learning_rate": 1.2675464898409772e-05,
      "loss": 0.1903,
      "step": 251
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.37185385823249817,
      "learning_rate": 1.1833741167229585e-05,
      "loss": 0.2334,
      "step": 252
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.37147536873817444,
      "learning_rate": 1.1019174633389073e-05,
      "loss": 0.1851,
      "step": 253
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.3073369860649109,
      "learning_rate": 1.0232016201762696e-05,
      "loss": 0.1742,
      "step": 254
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.3580327332019806,
      "learning_rate": 9.472508334910945e-06,
      "loss": 0.1482,
      "step": 255
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.2702614963054657,
      "learning_rate": 8.740884978396357e-06,
      "loss": 0.134,
      "step": 256
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.3536052405834198,
      "learning_rate": 8.037371488723078e-06,
      "loss": 0.1628,
      "step": 257
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.33474570512771606,
      "learning_rate": 7.3621845639218704e-06,
      "loss": 0.1799,
      "step": 258
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.3151313364505768,
      "learning_rate": 6.715532176802297e-06,
      "loss": 0.1306,
      "step": 259
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.3026529550552368,
      "learning_rate": 6.0976135108923636e-06,
      "loss": 0.1579,
      "step": 260
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.28832265734672546,
      "learning_rate": 5.508618899085583e-06,
      "loss": 0.1222,
      "step": 261
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.2903851866722107,
      "learning_rate": 4.948729765014004e-06,
      "loss": 0.1666,
      "step": 262
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.27255532145500183,
      "learning_rate": 4.418118567165763e-06,
      "loss": 0.1735,
      "step": 263
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.27535760402679443,
      "learning_rate": 3.916948745763937e-06,
      "loss": 0.1447,
      "step": 264
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.33175861835479736,
      "learning_rate": 3.4453746724232515e-06,
      "loss": 0.2684,
      "step": 265
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.28857848048210144,
      "learning_rate": 3.003541602600157e-06,
      "loss": 0.1157,
      "step": 266
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 0.33163362741470337,
      "learning_rate": 2.5915856308508345e-06,
      "loss": 0.2205,
      "step": 267
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.26560288667678833,
      "learning_rate": 2.2096336489111026e-06,
      "loss": 0.1146,
      "step": 268
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.30543285608291626,
      "learning_rate": 1.8578033066107393e-06,
      "loss": 0.2029,
      "step": 269
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.28464069962501526,
      "learning_rate": 1.5362029756348373e-06,
      "loss": 0.2268,
      "step": 270
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.31783533096313477,
      "learning_rate": 1.2449317161427943e-06,
      "loss": 0.2036,
      "step": 271
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.26123571395874023,
      "learning_rate": 9.840792462555426e-07,
      "loss": 0.1627,
      "step": 272
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.27778953313827515,
      "learning_rate": 7.537259144203779e-07,
      "loss": 0.1634,
      "step": 273
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.2921576499938965,
      "learning_rate": 5.539426746618336e-07,
      "loss": 0.1612,
      "step": 274
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.2636755108833313,
      "learning_rate": 3.847910647261754e-07,
      "loss": 0.0957,
      "step": 275
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.28180989623069763,
      "learning_rate": 2.463231871264626e-07,
      "loss": 0.166,
      "step": 276
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.2604859471321106,
      "learning_rate": 1.3858169309376444e-07,
      "loss": 0.1273,
      "step": 277
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.3202217221260071,
      "learning_rate": 6.159976943959089e-08,
      "loss": 0.2514,
      "step": 278
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.2544834315776825,
      "learning_rate": 1.5401128333669688e-08,
      "loss": 0.1342,
      "step": 279
    }
  ],
  "logging_steps": 1,
  "max_steps": 279,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.149308664883446e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
